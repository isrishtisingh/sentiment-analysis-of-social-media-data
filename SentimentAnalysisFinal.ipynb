{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "#### Sentiment analysis using 5 different classical NLP techniques\n",
    "<br>\n",
    "\n",
    "---\n",
    "<br>\n",
    "\n",
    "Techniques Used:\n",
    " 1. TextBlob <br>\n",
    "    a. Default method <br>\n",
    "    b. NaiveBayesClassifier <br>\n",
    "    c. Pattern analyzer <br>\n",
    " 2. Word dictionary based\n",
    " 3. Using pre-trained models (VADER)\n",
    " 4. Named Entity based (Targetted)\n",
    " 5. Custom trained \n",
    "\n",
    "<br>    \n",
    "   \n",
    "---\n",
    "<br>\n",
    "\n",
    "\n",
    "## 1. TextBlob\n",
    "\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from textblob.sentiments import NaiveBayesAnalyzer, PatternAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = 'Wuhan CoronaVirus Pictures that China does not want you to see Corona Update'     # news headline\n",
    "text2 = 'now the question is who will make China pay for the death sorrow destruction and disruption'   # top comments\n",
    "text3 = 'cries and mourning of millions of people will haunt China for sure.'   # top comments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Default Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.0, subjectivity=0.0)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob = TextBlob(text1)\n",
    "blob.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.0, subjectivity=0.0)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob = TextBlob(text2)\n",
    "blob.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.5, subjectivity=0.8888888888888888)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob = TextBlob(text3)\n",
    "blob.sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(classification='pos', p_pos=0.8025964603466035, p_neg=0.1974035396533971)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob = TextBlob(text1, analyzer=NaiveBayesAnalyzer())\n",
    "blob.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(classification='pos', p_pos=0.9528212793210686, p_neg=0.04717872067892939)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob = TextBlob(text2, analyzer=NaiveBayesAnalyzer())\n",
    "blob.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(classification='pos', p_pos=0.775269396124171, p_neg=0.22473060387583094)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob = TextBlob(text3, analyzer=NaiveBayesAnalyzer())\n",
    "blob.sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Pattern Analyzer (the default method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.5, subjectivity=0.8888888888888888)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob = TextBlob(text3, analyzer=PatternAnalyzer())\n",
    "blob.sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Word-Dictionary based\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total positive words found:  2718\n",
      "Total negative words found:  4911\n"
     ]
    }
   ],
   "source": [
    "pos_words = []\n",
    "neg_words = []\n",
    "\n",
    "#Ignoring neutral, both, and other polarities\n",
    "with open('data/subjclueslen1-HLTEMNLP05.tff') as file:\n",
    "    for line in file:\n",
    "        line_attrib = line.split()\n",
    "        word = line_attrib[2].split('=')[1] #2nd column in the file\n",
    "        polarity = line_attrib[-1].split('=')[1] #last column in the file\n",
    "        if polarity =='positive':\n",
    "            pos_words.append(word)\n",
    "        elif polarity=='negative':\n",
    "            neg_words.append(word)\n",
    "            \n",
    "print('Total positive words found: ',len(pos_words))\n",
    "print('Total negative words found: ',len(neg_words))\n",
    "\n",
    "#Write results to file for future use\n",
    "with open('pos_words.txt', mode='wt', encoding='utf-8') as myfile:\n",
    "    myfile.write('\\n'.join(pos_words))\n",
    "with open('neg_words.txt', mode='wt', encoding='utf-8') as myfile:\n",
    "    myfile.write('\\n'.join(neg_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total positive words found:  2720\n",
      "Total negative words found:  4922\n"
     ]
    }
   ],
   "source": [
    "pos_word_add = ['lifted', 'peace']\n",
    "for term in pos_word_add:\n",
    "    pos_words.append(term)\n",
    "\n",
    "neg_word_add = ['Taliban', 'war', 'Wuhan CoronaVirus', 'CoronaVirus', 'Corona', 'coronavirus','covid','corona','covid19','pandemic','lockdown']\n",
    "for term in neg_word_add:\n",
    "    neg_words.append(term)\n",
    "    \n",
    "print('Total positive words found: ',len(pos_words))\n",
    "print('Total negative words found: ',len(neg_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "def calc_sentiment_based_on_word_dict(text):\n",
    "    sentiment_score = 0\n",
    "    words = nltk.word_tokenize(text)\n",
    "    for word in words:\n",
    "        if word in pos_words:\n",
    "            print('pos:',word)\n",
    "            sentiment_score=sentiment_score+1\n",
    "        if word in neg_words:\n",
    "            print('neg:',word)\n",
    "            sentiment_score=sentiment_score-1\n",
    "    return sentiment_score/len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg: CoronaVirus\n",
      "pos: want\n",
      "neg: Corona\n",
      "The sentiment score of this text is: -0.08\n"
     ]
    }
   ],
   "source": [
    "sentiment = calc_sentiment_based_on_word_dict(text1)\n",
    "print('The sentiment score of this text is: {:.2f}'.format(sentiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos: will\n",
      "neg: death\n",
      "neg: sorrow\n",
      "neg: destruction\n",
      "neg: disruption\n",
      "The sentiment score of this text is: -0.19\n"
     ]
    }
   ],
   "source": [
    "sentiment = calc_sentiment_based_on_word_dict(text2)\n",
    "print('The sentiment score of this text is: {:.2f}'.format(sentiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos: will\n",
      "neg: haunt\n",
      "pos: sure\n",
      "The sentiment score of this text is: 0.08\n"
     ]
    }
   ],
   "source": [
    "sentiment = calc_sentiment_based_on_word_dict(text3)\n",
    "print('The sentiment score of this text is: {:.2f}'.format(sentiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg: Taliban\n",
      "pos: peace\n",
      "neg: war\n",
      "The sentiment score of this text is: -0.05\n"
     ]
    }
   ],
   "source": [
    "text4 = 'He added that Taliban have seized control of all areas restoring peace and normalcy in the war torn country'\n",
    "sentiment = calc_sentiment_based_on_word_dict(text4)\n",
    "print('The sentiment score of this text is: {:.2f}'.format(sentiment) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.0, subjectivity=0.0)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob = TextBlob(text4, analyzer=PatternAnalyzer())\n",
    "blob.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(classification='pos', p_pos=0.9969839734765537, p_neg=0.0030160265234435817)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob = TextBlob(text4, analyzer=NaiveBayesAnalyzer())\n",
    "blob.sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Using pre-trained models like VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1st Text 1 :\n",
      "Overall sentiment dictionary is :  {'neg': 0.092, 'neu': 0.908, 'pos': 0.0, 'compound': -0.0572}\n",
      "sentence was rated as  9.2 % Negative\n",
      "sentence was rated as  90.8 % Neutral\n",
      "sentence was rated as  0.0 % Positive\n",
      "Sentence Overall Rated As Negative\n",
      "\n",
      "2nd Text 2 :\n",
      "Overall sentiment dictionary is :  {'neg': 0.575, 'neu': 0.425, 'pos': 0.0, 'compound': -0.9313}\n",
      "sentence was rated as  57.49999999999999 % Negative\n",
      "sentence was rated as  42.5 % Neutral\n",
      "sentence was rated as  0.0 % Positive\n",
      "Sentence Overall Rated As Negative\n",
      "\n",
      "3rd Text 3 :\n",
      "Overall sentiment dictionary is :  {'neg': 0.446, 'neu': 0.43, 'pos': 0.124, 'compound': -0.7184}\n",
      "sentence was rated as  44.6 % Negative\n",
      "sentence was rated as  43.0 % Neutral\n",
      "sentence was rated as  12.4 % Positive\n",
      "Sentence Overall Rated As Negative\n"
     ]
    }
   ],
   "source": [
    "# import SentimentIntensityAnalyzer class\n",
    "# from vaderSentiment.vaderSentiment module.\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "\n",
    "def sentiment_scores(sentence):\n",
    "\n",
    "    # Create a SentimentIntensityAnalyzer object.\n",
    "    sent_obj = SentimentIntensityAnalyzer()\n",
    "\n",
    "    # polarity_scores method of SentimentIntensityAnalyzer\n",
    "    # object gives a sentiment dictionary.\n",
    "    # which contains pos, neg, neu, and compound scores.\n",
    "    sentiment_dict = sent_obj.polarity_scores(sentence)\n",
    "    \n",
    "    print(\"Overall sentiment dictionary is : \", sentiment_dict)\n",
    "    print(\"sentence was rated as \", sentiment_dict['neg']*100, \"% Negative\")\n",
    "    print(\"sentence was rated as \", sentiment_dict['neu']*100, \"% Neutral\")\n",
    "    print(\"sentence was rated as \", sentiment_dict['pos']*100, \"% Positive\")\n",
    "\n",
    "    print(\"Sentence Overall Rated As\", end = \" \")\n",
    "\n",
    "    # decide sentiment as positive, negative and neutral\n",
    "    if sentiment_dict['compound'] >= 0.05 :\n",
    "        print(\"Positive\")\n",
    "\n",
    "    elif sentiment_dict['compound'] <= - 0.05 :\n",
    "        print(\"Negative\")\n",
    "\n",
    "    else :\n",
    "        print(\"Neutral\")\n",
    "\n",
    "\n",
    "\n",
    "# Driver code\n",
    "if __name__ == \"__main__\" :\n",
    "    \n",
    "    print(\"\\n1st Text 1 :\")\n",
    "    sentiment_scores(text1)\n",
    "\n",
    "    print(\"\\n2nd Text 2 :\")\n",
    "    sentiment_scores(text2)\n",
    "\n",
    "    print(\"\\n3rd Text 3 :\")\n",
    "    sentiment_scores(text3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Named Entity based Sentiment Analysis\n",
    "\n",
    "\n",
    "#### Named Entity Recognition SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U SpaCy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data\\wion.txt\")\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def spacy_ner(text):\n",
    "    text = text.replace('\\n', ' ')\n",
    "    doc = nlp(text)\n",
    "    entities = []\n",
    "    labels = []\n",
    "    position_start = []\n",
    "    position_end = []\n",
    "\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in ['PERSON','ORG','GPE']:\n",
    "            entities.append(ent)\n",
    "            labels.append(ent.label_)\n",
    "    return entities,labels\n",
    "\n",
    "def fit_ner(df):\n",
    "    \"\"\"The dataframe should have a column named 'text'\"\"\"\n",
    "    print('Fitting Spacy NER model...')\n",
    "    ner = df['text'].apply(spacy_ner)\n",
    "    ner_org = {}\n",
    "    ner_per = {}\n",
    "    ner_gpe = {}\n",
    "\n",
    "    for x in ner:\n",
    "        #print(list(x))\n",
    "        for entity, label in zip(x[0],x[1]):\n",
    "            #print(type(entity.text))\n",
    "            if label =='ORG':\n",
    "                ner_org[entity.text] = ner_org.get(entity.text,0) + 1\n",
    "            elif label=='PERSON':\n",
    "                ner_per[entity.text] = ner_per.get(entity.text,0) + 1\n",
    "            else:\n",
    "                ner_gpe[entity.text] = ner_gpe.get(entity.text,0) + 1\n",
    "\n",
    "    return {'ORG':ner_org,'PER':ner_per,'GPE':ner_gpe}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Spacy NER model...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "news_df = pd.read_csv(\"data\\wion.txt\")\n",
    "named_entities = fit_ner(news_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Organization Named Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Wuhan CoronaVirus  Pictures': 1,\n",
       " 'Taliban': 1,\n",
       " 'Central China Floods Death': 1,\n",
       " 'Mouse': 1}"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "named_entities['ORG']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Geopolitical Named Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'China': 6,\n",
       " 'US': 3,\n",
       " 'Israel': 2,\n",
       " 'Gaza': 2,\n",
       " 'Kabul': 1,\n",
       " 'Wuhan': 1,\n",
       " 'Henan province': 1,\n",
       " 'Australia': 1,\n",
       " 'Afghanistan': 1,\n",
       " 'India': 1}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "named_entities['GPE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Named Entities based Sentiment Analysis (Targetted)\n",
    "\n",
    "Targetted by finding sentences containing the named entities and performing sentiment analysis on those sentences one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "import nltk\n",
    "\n",
    "def get_keyword_sentences(text,keyword):\n",
    "    \"\"\"Extract sentences containing the Named Entity/Keyword\"\"\"\n",
    "    text = text.replace('\\n', ' ')\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "    sent_of_interest = []\n",
    "    for sent in sentences:\n",
    "        if keyword in sent.lower():\n",
    "            sent_of_interest.append(sent)\n",
    "    return sent_of_interest if len(sent_of_interest)>0 else False\n",
    "\n",
    "\n",
    "def get_sentiment(list_of_sent):\n",
    "    \"\"\" Extract sentiment from a sentence using TextBlob Pattern Analyzer\"\"\"\n",
    "    sentiment = 0\n",
    "    count = 0\n",
    "\n",
    "    if list_of_sent !=False:\n",
    "        for sent in list_of_sent:\n",
    "            blob = TextBlob(sent)\n",
    "            sentiment += blob.sentiment.polarity\n",
    "            count += len(sent)\n",
    "    return sentiment/count if count!=0 else 0  \n",
    "\n",
    "def extract_sentiment(df,keywords,top=10):\n",
    "    \"\"\"df: data for the cluster, keywords: ner for that cluster\"\"\"\n",
    "    print('Extracting Sentiments using TextBlob...')\n",
    "    keywords = sorted(keywords.items(),key=lambda x: x[1],reverse=True)\n",
    "    sentiment_dict = {}\n",
    "    for keyword,count in keywords[:top]:\n",
    "        df['sentences'] = df['text'].apply(get_keyword_sentences,keyword=keyword.lower())\n",
    "        keyword_sentiment = df['sentences'].apply(get_sentiment).sum()\n",
    "        sentiment_dict[keyword] = keyword_sentiment\n",
    "    return sentiment_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Geopolitical Entities according to their sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Sentiments using TextBlob...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Kabul', 0.0030120481927710845),\n",
       " ('US', 0.0026638629322774145),\n",
       " ('Israel', 0.0),\n",
       " ('Gaza', 0.0),\n",
       " ('Wuhan', 0.0),\n",
       " ('Henan province', 0.0),\n",
       " ('Australia', 0.0),\n",
       " ('Afghanistan', 0.0),\n",
       " ('China', -0.00203962703962704),\n",
       " ('India', -0.008483697372586262)]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords = named_entities['GPE']\n",
    "sentiment_result_org = extract_sentiment(news_df,keywords,top=10)\n",
    "sentiment_result_org\n",
    "sorted(sentiment_result_org.items(),key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Organizations according to their sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Sentiments using TextBlob...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Taliban', 0.0030120481927710845),\n",
       " ('Wuhan CoronaVirus  Pictures', 0.0),\n",
       " ('Central China Floods Death', 0.0),\n",
       " ('Mouse', 0.0)]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords = named_entities['ORG']\n",
    "sentiment_result_org = extract_sentiment(news_df,keywords,top=5)\n",
    "sentiment_result_org\n",
    "sorted(sentiment_result_org.items(),key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset used for NER based sentiment analysis are the following News Headlines from the channel WION."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Wuhan CoronaVirus  Pictures that China does not want you to see  Corona Update'],\n",
       "       ['This Syrian child  message to the world will break your heart'],\n",
       "       ['What happened when Trump did not know he was already Some Unseen Footages  US President'],\n",
       "       ['The interview China tried to hide Wuhan Coronavirus  Dr. Ai Fen'],\n",
       "       ['Israel Palestinian clashes Thousands flee their homes in Gaza  Hamas World English News'],\n",
       "       ['Violence escalates between Israel Gaza'],\n",
       "       [\"Ground Report Kabul first morning under Taliban rule Afghan's national flag lowered\"],\n",
       "       ['Israel Palestine clashes  Gaza Violence  Hamas Rocket Attacks World News'],\n",
       "       ['WION Wideangle INDIA SOUTH CHINA Sea TROUBLED WATERS'],\n",
       "       ['How China exported the virus  Wuhan coronavirus'],\n",
       "       ['Central China Floods Death toll goes up to 33 thousands evacuated from Henan province'],\n",
       "       ['Mouse plague in Australia'],\n",
       "       ['Trump drops Largest Non Nuclear Bomb In Afghanistan'],\n",
       "       ['US Election 2020 Results Donald Trump vs Joe Biden  US Presidential Election Results Live'],\n",
       "       ['A warning to Xi Jinping from China  hawkish generals'],\n",
       "       ['To better understand each other Indian  US armies conduct joint military exercise'],\n",
       "       ['Wuhan Coronavirus China sued for 20 trillion in damages'],\n",
       "       ['The Belt Road initiative'],\n",
       "       ['Uighur genocide Speak up Islamic world'],\n",
       "       ['Angela Merkel says Europe Allowed India to become a pharma hub'],\n",
       "       ['Now the question is who will make China pay for the death sorrow destruction and disruption'],\n",
       "       ['Cries and mourning of millions of people will haunt China for sure']],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"data\\wion.txt\")\n",
    "df.values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Custom Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import scipy\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlxtend\n",
    "from textblob import TextBlob\n",
    "from sklearn import metrics\n",
    "from mlxtend.plotting import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wuhan CoronaVirus  Pictures that China does no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This Syrian child  message to the world will b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What happened when Trump did not know he was a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The interview China tried to hide Wuhan Corona...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Israel Palestinian clashes Thousands flee thei...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  Wuhan CoronaVirus  Pictures that China does no...\n",
       "1  This Syrian child  message to the world will b...\n",
       "2  What happened when Trump did not know he was a...\n",
       "3  The interview China tried to hide Wuhan Corona...\n",
       "4  Israel Palestinian clashes Thousands flee thei..."
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"data\\wion.txt\")\n",
    "data.shape\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 text  polarity\n",
      "0   Wuhan CoronaVirus  Pictures that China does no...  0.000000\n",
      "1   This Syrian child  message to the world will b...  0.000000\n",
      "2   What happened when Trump did not know he was a...  0.000000\n",
      "3   The interview China tried to hide Wuhan Corona...  0.000000\n",
      "4   Israel Palestinian clashes Thousands flee thei...  0.000000\n",
      "5              Violence escalates between Israel Gaza  0.000000\n",
      "6   Ground Report Kabul first morning under Taliba...  0.250000\n",
      "7   Israel Palestine clashes  Gaza Violence  Hamas...  0.000000\n",
      "8   WION Wideangle INDIA SOUTH CHINA Sea TROUBLED ... -0.500000\n",
      "9     How China exported the virus  Wuhan coronavirus  0.000000\n",
      "10  Central China Floods Death toll goes up to 33 ...  0.000000\n",
      "11                          Mouse plague in Australia  0.000000\n",
      "12  Trump drops Largest Non Nuclear Bomb In Afghan...  0.000000\n",
      "13  US Election 2020 Results Donald Trump vs Joe B...  0.136364\n",
      "14  A warning to Xi Jinping from China  hawkish ge...  0.000000\n",
      "15  To better understand each other Indian  US arm...  0.091667\n",
      "16  Wuhan Coronavirus China sued for 20 trillion i...  0.000000\n",
      "17                           The Belt Road initiative  0.000000\n",
      "18             Uighur genocide Speak up Islamic world  0.000000\n",
      "19  Angela Merkel says Europe Allowed India to bec...  0.000000\n",
      "20  Now the question is who will make China pay fo...  0.000000\n",
      "21  Cries and mourning of millions of people will ...  0.500000\n"
     ]
    }
   ],
   "source": [
    "data['polarity'] = data['text'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### the above dataset is too small to be useful in this case, so we'll download comments  of a news channel video on Covid19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I can't even say china should be ashamed.becau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Each nation should take action against China. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What a brave woman!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Praying shes still alive. Salute to these brav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is stupid because it was made in 2019 ,an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Comment\n",
       "0  I can't even say china should be ashamed.becau...\n",
       "1  Each nation should take action against China. ...\n",
       "2                               What a brave woman!!\n",
       "3  Praying shes still alive. Salute to these brav...\n",
       "4  This is stupid because it was made in 2019 ,an..."
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"data\\comment11.csv\")\n",
    "#data.shape\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Comment  polarity\n",
      "0   I can't even say china should be ashamed.becau...  0.000000\n",
      "1   Each nation should take action against China. ...  0.125000\n",
      "2                                What a brave woman!!  1.000000\n",
      "3   Praying shes still alive. Salute to these brav...  0.450000\n",
      "4   This is stupid because it was made in 2019 ,an... -0.800000\n",
      "..                                                ...       ...\n",
      "95  Why is everything a cover up. Why can't people...  0.136364\n",
      "96  Be your brother's  keeper.  Stand up for each ... -0.125000\n",
      "97  It has no characteristics of a vaccine.  But y...  0.000000\n",
      "98  Why hasn’t the world asked China too pay compe... -0.055556\n",
      "99  What I am interested in right now that why We ...  0.183929\n",
      "\n",
      "[100 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "data['polarity'] = data['Comment'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "data['pol_cat']  = 0    #categorical polarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Continuous to categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "      <th>polarity</th>\n",
       "      <th>pol_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"Missing\" in China means \"tortured and murdere...</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fauci enabled Communist Party cover-up of covid.</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>its completely true as all developed countries...</td>\n",
       "      <td>0.225000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fauci lied. Millions died.</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>She got missing. I bet she they  killed  her. ...</td>\n",
       "      <td>-0.133333</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Comment  polarity  pol_cat\n",
       "0  \"Missing\" in China means \"tortured and murdere...  0.150000        1\n",
       "1   Fauci enabled Communist Party cover-up of covid.  0.000000       -1\n",
       "2  its completely true as all developed countries...  0.225000        1\n",
       "3                         Fauci lied. Millions died.  0.000000       -1\n",
       "4  She got missing. I bet she they  killed  her. ... -0.133333       -1"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data['pol_cat'][data.polarity == 0] = 0\n",
    "data['pol_cat'][data.polarity > 0] = 1\n",
    "data['pol_cat'][data.polarity <= 0] = -1\n",
    "\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1    73\n",
       " 1    27\n",
       "Name: pol_cat, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['pol_cat'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create separate dataframes for Negative,Positive & Neutral comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pos = data[data['pol_cat'] == 1]\n",
    "data_pos = data_pos.reset_index(drop = True)\n",
    "\n",
    "data_neg = data[data['pol_cat'] == -1]\n",
    "data_neg = data_neg.reset_index(drop = True)\n",
    "\n",
    "# data_neutral = data[data['pol_cat'] == 0]\n",
    "# data_neutral = data_neutral.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, 3)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "      <th>polarity</th>\n",
       "      <th>pol_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"Missing\" in China means \"tortured and murdere...</td>\n",
       "      <td>0.1500</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>its completely true as all developed countries...</td>\n",
       "      <td>0.2250</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MISSING HUH?!! MORE LIKE SILENCED BY THE CHINE...</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Blaming yourself is better.</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What a brave woman!!</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Comment  polarity  pol_cat\n",
       "0  \"Missing\" in China means \"tortured and murdere...    0.1500        1\n",
       "1  its completely true as all developed countries...    0.2250        1\n",
       "2  MISSING HUH?!! MORE LIKE SILENCED BY THE CHINE...    0.0625        1\n",
       "3                        Blaming yourself is better.    0.5000        1\n",
       "4                               What a brave woman!!    1.0000        1"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(73, 3)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_neg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "      <th>polarity</th>\n",
       "      <th>pol_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fauci enabled Communist Party cover-up of covid.</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fauci lied. Millions died.</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>She got missing. I bet she they  killed  her. ...</td>\n",
       "      <td>-0.133333</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>🔥</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHINA BRING THIS DR BACK!!!!</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Comment  polarity  pol_cat\n",
       "0   Fauci enabled Communist Party cover-up of covid.  0.000000       -1\n",
       "1                         Fauci lied. Millions died.  0.000000       -1\n",
       "2  She got missing. I bet she they  killed  her. ... -0.133333       -1\n",
       "3                                                  🔥  0.000000       -1\n",
       "4                       CHINA BRING THIS DR BACK!!!!  0.000000       -1"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_neg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Dead or n a camp they don't play about that snitch shit\""
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_neg['Comment'][40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1    73\n",
       " 1    27\n",
       "Name: pol_cat, dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.pol_cat.value_counts().plot.bar()\n",
    "data.pol_cat.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Comment'] = data['Comment'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    \"missing\" in china means \"tortured and murdere...\n",
       "1     fauci enabled communist party cover-up of covid.\n",
       "2    its completely true as all developed countries...\n",
       "3                           fauci lied. millions died.\n",
       "4    she got missing. i bet she they  killed  her. ...\n",
       "Name: Comment, dtype: object"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Comment'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\srish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\srish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "import string\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"missing\" in china means \"tortured and murdered\". \\ni will keep you in my prayers doctor ai fen. china will pay it\\'s dues for sure.'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "data['Comment'] = data['Comment'].str.strip()\n",
    "train = data.copy()\n",
    "train['Comment'] = train['Comment'].str.strip()\n",
    "\n",
    "train['Comment'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(line):\n",
    "    word_tokens = word_tokenize(line)\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "    return \" \".join(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['stop_comments'] = data['Comment'].apply(lambda x : remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "      <th>polarity</th>\n",
       "      <th>pol_cat</th>\n",
       "      <th>stop_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"missing\" in china means \"tortured and murdere...</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>1</td>\n",
       "      <td>`` missing '' china means `` tortured murdered...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fauci enabled communist party cover-up of covid.</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1</td>\n",
       "      <td>fauci enabled communist party cover-up covid .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>its completely true as all developed countries...</td>\n",
       "      <td>0.225000</td>\n",
       "      <td>1</td>\n",
       "      <td>completely true developed countries overcome y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fauci lied. millions died.</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1</td>\n",
       "      <td>fauci lied . millions died .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>she got missing. i bet she they  killed  her. ...</td>\n",
       "      <td>-0.133333</td>\n",
       "      <td>-1</td>\n",
       "      <td>got missing . bet killed . ’ want talk . soone...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Comment  polarity  pol_cat  \\\n",
       "0  \"missing\" in china means \"tortured and murdere...  0.150000        1   \n",
       "1   fauci enabled communist party cover-up of covid.  0.000000       -1   \n",
       "2  its completely true as all developed countries...  0.225000        1   \n",
       "3                         fauci lied. millions died.  0.000000       -1   \n",
       "4  she got missing. i bet she they  killed  her. ... -0.133333       -1   \n",
       "\n",
       "                                       stop_comments  \n",
       "0  `` missing '' china means `` tortured murdered...  \n",
       "1     fauci enabled communist party cover-up covid .  \n",
       "2  completely true developed countries overcome y...  \n",
       "3                       fauci lied . millions died .  \n",
       "4  got missing . bet killed . ’ want talk . soone...  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(data['stop_comments'],data['pol_cat'],test_size = 0.2,random_state = 324)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80,)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20,)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1    73\n",
       " 1    27\n",
       "Name: pol_cat, dtype: int64"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['pol_cat'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer \n",
    "vect = CountVectorizer()\n",
    "tf_train = vect.fit_transform(X_train)\n",
    "tf_test = vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 489)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print vocabulary\n",
    "#print(vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import the logistic regression classifer and fit on the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(tf_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy score on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(tf_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy score on test dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "lr.score(tf_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making predictions on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected = y_test\n",
    "predicted = lr.predict(tf_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot confusion matrix for the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  5]\n",
      " [ 0 15]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQMAAAEGCAYAAABhHPB4AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAOYElEQVR4nO3de5CV9XnA8e8jlPGGVzCJuwiIii6OMQHUNtZRxygoonV0AtomRKPxFmNtVNJc1OpUrXZSq2m9xZqJ8ZrEMeIFL9PRhqpAbbxFjShaWWhEaZGqCbo+/WN/4AmF3bOW97wLfD8zO3vec86e9zmCX973Pe/ZE5mJJG1U9wCS+gdjIAkwBpIKYyAJMAaSioF1D9BoyJAhOXz4iLrHUB/893vv1z2C+mDxotd5+7+WxOpu61cxGD58BLOemFv3GOqDGc8urHsE9cG5x05c423uJkgCjIGkwhhIAoyBpMIYSAKMgaTCGEgCjIGkwhhIAoyBpMIYSAKMgaTCGEgCjIGkwhhIAoyBpMIYSAKMgaTCGEgCjIGkwhhIAoyBpMIYSAKMgaTCGEgCjIGkwhhIAoyBpMIYSAKMgaTCGEgCjIGkwhhIAoyBpMIYSAKMgaTCGEgCjIGkwhhIAoyBpMIYSAKMgaTCGEgCjIGkwhhIAoyBpMIYSAKMgaTCGEgCYGDdA6zPHph5P9846+t0dXUx7fivcPY50+seSb049dC92Xizzdloo40YMGAgl958X90jtUylMYiICcAVwADg+sy8pMr19SddXV2cecZp3HPfg7S1t7PvPuOZNGkyu3V01D2aenH+tXewxdbb1D1Gy1W2mxARA4DvAxOBDmBqRGww/yfMmT2bUaN2YuSOOzJo0CCO+cIUZtx9V91jSWtU5TGDvYB5mflKZi4HbgWOqHB9/crChZ20tw9budzW1k5nZ2eNE6kpEVx06lTOOXYCD/70prqnaakqdxPagNcblhcAe696p4g4CTgJYNgOO1Q4jtS7C//pTrbd7lMsXfImF548hbYRO9Exdp+6x2qJ2l9NyMxrM3NcZo4bOmRo3eOsNdtv38aCBR+1sLNzAW1tbTVOpGZsu92nANhymyHsdeBE5j33y3oHaqEqY9AJDGtYbi/XbRDGjR/PvHkv8er8+Sxfvpw7bruVwyZNrnss9eC3773Le+/8z8rLTz32CMNGja55qtapcjdhDrBzRIykOwJTgGMrXF+/MnDgQL53xVUcftghdHV18aVpx9MxZkzdY6kHS99azGVnnQB0vxq078Qj+cznDqh5qtapLAaZ+UFEnA7MpPulxRsy87mq1tcfTZh4KBMmHlr3GGrSJ9qHc/ntD9U9Rm0qPc8gM+8F7q1yHZLWjtoPIErqH4yBJMAYSCqMgSTAGEgqjIEkwBhIKoyBJMAYSCqMgSTAGEgqjIEkwBhIKoyBJMAYSCqMgSTAGEgqjIEkwBhIKoyBJMAYSCqMgSTAGEgqjIEkwBhIKoyBJMAYSCrW+FmLEbEMyBWL5XuWy5mZW1Q8m6QWWmMMMnNwKweRVK+mdhMiYt+I+HK5PCQiRlY7lqRW6zUGEXEecC7wzXLVIOCmKoeS1HrNbBn8CTAZeAcgMxcC7kJI65lmYrA8M5NyMDEiNqt2JEl1aCYGt0fENcBWEXEi8BBwXbVjSWq1Nb6asEJmXh4RnwfeBnYBvpuZD1Y+maSW6jUGxTPAJnTvKjxT3TiS6tLMqwlfAWYDRwFHA49HxPFVDyaptZrZMjgb+ExmvgUQEdsC/wrcUOVgklqrmQOIbwHLGpaXleskrUd6em/CWeXiPOCJiLiL7mMGRwBPt2A2SS3U027CihOLXi5fK9xV3TiS6tLTG5UuaOUgkurV6wHEiBgKnAOMATZecX1mHljhXJJarJkDiD8GXgBGAhcArwJzKpxJUg2aicG2mfkD4P3MfCQzjwfcKpDWM82cZ/B++b4oIg4DFgLbVDeSpDo0E4OLImJL4C+AK4EtgD+vdCpJLdfMG5VmlItLgQOqHUdSXXo66ehKPvqFqP9HZp5RyURap/zZl/+67hHUB797ddEab+tpy2Du2h9FUn/V00lHP2zlIJLq5YeoSAKMgaTCGEgCmvtNR7tExMMR8WxZ3iMivl39aJJaqZktg+vo/gCV9wEy82lgSpVDSWq9ZmKwaWbOXuW6D6oYRlJ9monBmxExio8+ROVoYM1nLkhaJzXz3oTTgGuBXSOiE5gP/GmlU0lquWbem/AKcFD5WLWNMnNZbz8jad3TzG86+u4qywBk5l9VNJOkGjSzm/BOw+WNgUnA89WMI6kuzewm/G3jckRcDsysbCJJtfg4ZyBuCrSv7UEk1auZYwbP8NHvNRgADAU8XiCtZ5o5ZjCp4fIHwG8y05OOpPVMjzGIiAHAzMzctUXzSKpJj8cMMrMLeDEidmjRPJJq0sxuwtbAcxExm4aXGTNzcmVTSWq5ZmLwncqnkFS7ZmJwaGae23hFRFwKPFLNSJLq0Mx5Bp9fzXUT1/YgkurV0+cmnAKcCuwYEU833DQYmFX1YJJaq6fdhJuB+4CLgekN1y/LzCWVTiWp5Xr63ISldH+k2tTWjSOpLv52ZEmAMZBUGANJgDGQVBgDSYAxkFQYA0mAMZBUGANJgDGQVBgDSYAxkFQYA0mAMZBUGANJgDGQVBgDSYAxkFQYA0mAMZBUGANJgDGQVBgDSYAxqNQDM+9njzGjGbPrTlz2N5fUPY5W4+rzjuO1hy9m7h1/ufK6b331UF6eeRGP3zqdx2+dziH7dtQ4Yes088GrH0tE3ABMAt7IzN2rWk9/1dXVxZlnnMY99z1IW3s7++4znkmTJrNbx4bxF2td8aO7H+fq2x7h+gu/+HvXX3nTP/N3P3q4pqnqUeWWwY3AhAofv1+bM3s2o0btxMgdd2TQoEEc84UpzLj7rrrH0ipmPfkyS5a+W/cY/UJlMcjMR4EN9jMZFy7spL192MrltrZ2Ojs7a5xIfXHylP2Yfds3ufq849hq8CZ1j9MStR8ziIiTImJuRMxd/ObiuseRuO6Of6Hj8PPZe8ol/Oebb3PJWUfVPVJL1B6DzLw2M8dl5rihQ4bWPc5as/32bSxY8PrK5c7OBbS1tdU4kZr1xpJlfPhhkpnc8LNZjNt9eN0jtUTtMVhfjRs/nnnzXuLV+fNZvnw5d9x2K4dNmlz3WGrCJ4dssfLyEQd+ml+9vKjGaVqnslcTNnQDBw7ke1dcxeGHHUJXVxdfmnY8HWPG1D2WVvHDi6fxx2N3ZshWmzPv/gu58Op72W/szuwxup3M5LVFS/jaRbfUPWZLRGZW88ARtwD7A0OA3wDnZeYPevqZsWPH5awn5lYyj6qx9fjT6x5BffC7F2/nw3ffiNXdVtmWQWZOreqxJa19HjOQBBgDSYUxkAQYA0mFMZAEGANJhTGQBBgDSYUxkAQYA0mFMZAEGANJhTGQBBgDSYUxkAQYA0mFMZAEGANJhTGQBBgDSYUxkAQYA0mFMZAEGANJhTGQBBgDSYUxkAQYA0mFMZAEGANJhTGQBBgDSYUxkAQYA0mFMZAEGANJhTGQBBgDSYUxkAQYA0mFMZAEGANJhTGQBBgDSYUxkAQYA0mFMZAEGANJhTGQBEBkZt0zrBQRi4HX6p6jAkOAN+seQn2yvv6ZDc/Moau7oV/FYH0VEXMzc1zdc6h5G+KfmbsJkgBjIKkwBq1xbd0DqM82uD8zjxlIAtwykFQYA0mAMahUREyIiBcjYl5ETK97HvUuIm6IiDci4tm6Z2k1Y1CRiBgAfB+YCHQAUyOio96p1IQbgQl1D1EHY1CdvYB5mflKZi4HbgWOqHkm9SIzHwWW1D1HHYxBddqA1xuWF5TrpH7JGEgCjEGVOoFhDcvt5TqpXzIG1ZkD7BwRIyNiEDAF+HnNM0lrZAwqkpkfAKcDM4Hngdsz87l6p1JvIuIW4DFgdEQsiIgT6p6pVTwdWRLgloGkwhhIAoyBpMIYSAKMgaTCGGygImL/iJhRLk/u6V2VEbFVRJz6MdZxfkR8o9nrV7nPjRFxdB/WNWJDfKfh2mQM1jPl3ZJ9kpk/z8xLerjLVkCfY6B1izFYR5R/+V6IiB9HxPMR8ZOI2LTc9mpEXBoRTwLHRMTBEfFYRDwZEXdExOblfhPKYzwJHNXw2NMi4qpy+RMRcWdEPFW+/gi4BBgVEb+MiMvK/c6OiDkR8XREXNDwWN+KiF9HxC+A0U08rxPL4zwVET9d8ZyKgyJibnm8SeX+AyLisoZ1f/X/+99W3YzBumU08A+ZuRvwNr//r/VbmflZ4CHg28BBZXkucFZEbAxcBxwOjAU+uYZ1/D3wSGZ+Gvgs8BwwHXg5M/fMzLMj4mBgZ7rfpr0nMDYi9ouIsXSfdr0ncCgwvonn9LPMHF/W9zzQeMbfiLKOw4Cry3M4AViamePL458YESObWI96MbDuAdQnr2fmrHL5JuAM4PKyfFv5vg/dv0xlVkQADKL79NpdgfmZ+RJARNwEnLSadRwIfBEgM7uApRGx9Sr3Obh8/XtZ3pzuOAwG7szMd8s6mnkvxu4RcRHduyKb03369gq3Z+aHwEsR8Up5DgcDezQcT9iyrPvXTaxLPTAG65ZVzx1vXH6nfA/gwcyc2njHiNhzLc4RwMWZec0q6zjzYzzWjcCRmflUREwD9m+4bXXPN4CvZWZjNIiIER9j3WrgbsK6ZYeI+MNy+VjgF6u5z+PA5yJiJ4CI2CwidgFeAEZExKhyv6mr+VmAh4FTys8OiIgtgWV0/6u/wkzg+IZjEW0RsR3wKHBkRGwSEYPp3iXpzWBgUUT8AXDcKrcdExEblZl3BF4s6z6l3J+I2CUiNmtiPeqFMVi3vAicFhHPA1sD/7jqHTJzMTANuCUinqbsImTmb+neLbinHEB8Yw3r+DpwQEQ8A/wb0JGZb9G92/FsRFyWmQ8ANwOPlfv9BBicmU/SvbvyFHAf3W/j7s13gCeAWXQHq9F/ALPLY51cnsP1wK+AJ8tLidfgFu5a4bsW1xFlM3hGZu5e9yxaP7llIAlwy0BS4ZaBJMAYSCqMgSTAGEgqjIEkAP4XC8Y4XaWIhCQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cf = metrics.confusion_matrix(expected,predicted,labels = [1,-1])\n",
    "print(cf)\n",
    "## this command is only for google collab \n",
    "## in the below figure \" 1 is actually showing -1 and 0 is actually showing 1\"\n",
    "fig, ax = plot_confusion_matrix(conf_mat = cf)\n",
    "                                \n",
    "## It canbe used in any platform\n",
    "# fig, ax = plot_confusion_matrix(conf_mat = cf, class_names = [1,-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.75      1.00      0.86        15\n",
      "           1       0.00      0.00      0.00         5\n",
      "\n",
      "    accuracy                           0.75        20\n",
      "   macro avg       0.38      0.50      0.43        20\n",
      "weighted avg       0.56      0.75      0.64        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(expected, predicted))\n",
    "# print(metrics.confusion_matrix(expected,predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find f1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42857142857142855"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(expected, predicted, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I can't even say china should be ashamed.becau...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Each nation should take action against China. ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What a brave woman!!</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Praying shes still alive. Salute to these brav...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is stupid because it was made in 2019 ,an...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Why. How come UK colonise India for 2 hundreds...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>🔥</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>So you are the one that discovered the delta v...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I'm getting tired of this. It's like having sa...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>She is missing? She is dead along with million...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>In the mean time there was a new addition of a...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>The world should blast the china lab...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>God bless the matyre…May God protect u!!!</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>China still wants to rule the world?</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>People should have acted then when she spoke up.</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>The world must make China pay</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>severance package for lying in an interview to...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>\"There are no facts, only interpretations.\"</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>China GOV caused this ENTIRE FUCKING SHIT BRUH...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>What a joke. She can't even qualify as a micro...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comments  polarity\n",
       "0   I can't even say china should be ashamed.becau...        -1\n",
       "1   Each nation should take action against China. ...        -1\n",
       "2                                What a brave woman!!        -1\n",
       "3   Praying shes still alive. Salute to these brav...        -1\n",
       "4   This is stupid because it was made in 2019 ,an...        -1\n",
       "5   Why. How come UK colonise India for 2 hundreds...        -1\n",
       "6                                                   🔥        -1\n",
       "7   So you are the one that discovered the delta v...        -1\n",
       "8   I'm getting tired of this. It's like having sa...        -1\n",
       "9   She is missing? She is dead along with million...        -1\n",
       "10  In the mean time there was a new addition of a...        -1\n",
       "11            The world should blast the china lab...        -1\n",
       "12          God bless the matyre…May God protect u!!!        -1\n",
       "13               China still wants to rule the world?        -1\n",
       "14   People should have acted then when she spoke up.        -1\n",
       "15                      The world must make China pay        -1\n",
       "16  severance package for lying in an interview to...        -1\n",
       "17        \"There are no facts, only interpretations.\"        -1\n",
       "18  China GOV caused this ENTIRE FUCKING SHIT BRUH...        -1\n",
       "19  What a joke. She can't even qualify as a micro...        -1"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "   \n",
    "# List1\n",
    "#data.head() \n",
    "#pdToList = list(data['text'])\n",
    "\n",
    "col_one_list = data['Comment'].tolist()\n",
    "col_one_arr = data['Comment'].to_numpy()\n",
    "#print(f\"\\ncol_one_list:\\n{col_one_list}\\ntype:{type(col_one_list)}\")\n",
    "\n",
    "   \n",
    "# List2\n",
    "polarity = predicted\n",
    "   \n",
    "# get the list of tuples from two lists and merge them by using zip().\n",
    "list_of_tuples = list(zip(col_one_list, polarity))\n",
    "   \n",
    " \n",
    " \n",
    "# Converting lists of tuples into pandas Dataframe\n",
    "df = pd.DataFrame(list_of_tuples,\n",
    "                  columns = ['comments', 'polarity'])\n",
    "    \n",
    "#Print comments and their polarity\n",
    "df\n",
    "\n",
    "#data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "There are various ways to perform sentiment analysis on textual data, here we used 5 of those techniques on YouTube videos' title and comment data of news channel, which I expected to have extreme polarity and sentiments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
